


### Deliverables

Create a folder in your `compjour-hw` repo named: `usajobs-midterm-1`.

For each exercise, create a separate file, e.g.

    |-- compjour-hw/
        |-- usajobs-midterm-2/
           |-- 2-1.py
           |-- 2-2.py
           (etc)
           |-- 2-15.py 


Some exercises may require you to produce a file. For example, exercise __2-7__ requires that you create a webpage. Store that webpage in the same directory as the script files, e.g.


    |-- compjour-hw/
        |-- usajobs-midterm-2/
           |-- 2-7.py
           |-- 2-7.html
           (etc)



## Exercises


### 1. * Download the job data for every country and save as JSON

Using the list of country codes found here:

[http://stash.compjour.org/data/usajobs/CountryCode.json](http://stash.compjour.org/data/usajobs/CountryCode.json){:.rawurl}

Query the USAJobs API for each country, then make a list of lists that looks like:

~~~py
[
  ['Aruba', 0],
  ['Antigua and Barbuda', 0],
  ['Afghanistan', 12],
  # ... etc
  ['Zimbabwe', 1],
]
~~~

Finally, __save__ this list as JSON-formatted data, to the path:

       data-hold/intl-jobcount.json


#### Takeaway

This (and Exercise 2) involve using one data source (a [list of countries](http://stash.compjour.org/data/usajobs/CountryCode.json)) and looping through it to gather data from another source (the USAJobs API, using the `Country` parameter). A relatively simple __if-statement__ is required to filter out countries that are _not_ the U.S.

After the loop finishes, the result should be a __list__ that contains one entry for every country; and each entry is itself a list, containing the country name and its jobs count. Using the [__json__ library and its `dumps()` method](https://docs.python.org/3/library/json.html), the list is serialized and saved as a textfile, `data-hold/domestic-jobcount.json`. Subsequent exercises can then load this datafile without having to redo the (very long) data fetching process.

So this exercise is mostly to make the next few exercises more convenient. However, make sure you understand how significant it is that a data structure from this exercise -- i.e. the __list__ -- can be saved as text, and re-loaded (using `json.loads()`), effectively allowing multiple programs to "talk" to each other with data. Doing this process well is the foundation of most real-world data projects.

#### Solution

<%= codepiece '/answers/usajobs-midterm/2-1.py' %>


### 2. Download the job data for every U.S. state and save as JSON

Similar to Exercise #1, except use:

[http://stash.compjour.org/data/usajobs/us-statecodes.json](http://stash.compjour.org/data/usajobs/us-statecodes.json){:.rawurl}

The list-of-lists will look something like:

~~~py
[
  ['Alaska', 231],
  ['California', 750],
  ['Iowa', 421]
]
~~~



And save the file to:

       data-hold/domestic-jobcount.json


#### Takeaway

Pretty much the same process as Exercise 1, except you don't need an if-statement, as the [us-statecodes.json](http://stash.compjour.org/data/usajobs/us-statecodes.json) file contains _only_ the state names you need, i.e. no filtering is required.


### 3. * Sum and print the job listings for foreign countries and for U.S. states

__Note:__ Exercises 1 and 2 must have completed successfully before you can do this exercise.

Loop through `data-hold/domestic-jobcount.json` and `data-hold/intl-jobcount.json`, and for each collection, tally the __sum__ of the job counts, and then print a line for each sum, i.e.

~~~py
There are YY international jobs.
There are XX domestic jobs.
~~~


#### Takeaway

A simple summation exercise. In the given solution, I provide two ways to think about it. The first is simply to think about it as a __loop__: what more is there to calculating a _sum_ than going through a list and adding up the numbers?

The second example [uses Python's built-in __sum()__ function](https://docs.python.org/3/library/functions.html#sum), which, when given a sequence of numbers (e.g. a list), will return their sum. In this scenario, we don't have a list of numbers, but a _list-of-lists_, so I use a list comprehension, which is a [" concise way to create lists"](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions):

~~~py
xlist = [['apples', 10], ['oranges', 20]]
ylist = [z[1] in for z in xlist]
~~~

If you find that hard to *comprehend*, feel free to go with the more verbose, but perfectly fine, standard __for-loop__:

~~~py
xlist = [['apples', 10], ['oranges', 20]]
ylist = []
for z in xlist:
    ylist.append(z[1])
~~~

<%= codepiece '/answers/usajobs-midterm/2-3.py' %>



### 4. Print the U.S. job states with fewer than 100 listings

__Note:__ Exercise 2 must have completed successfully -- i.e. `data-hold/domestic-jobcount.json` exists -- before you can do this exercise.


Open `data-hold/domestic-jobcount.json` and, for the states that have _fewer than_ 100 job listings, print a comma-separated list of the state's name and job listings count.







#### Result

~~~
Rhode Island,41
Iowa,85
Connecticut,75
Delaware,51
New Hampshire,65
Maine,51
Vermont,31
~~~

### 5. Print the foreign countries with more than 10 listings

Open `data-hold/intl-jobcount.json` and, for the countries that have _more than_ 10 job listings, print a comma-separated list of the country's name and job listings count.

### 6. Print the job counts for the top 10 non-US countries, and sum all others

Similar to Exercise 5, read `data-hold/intl-jobcount.json` and print out the top 10 countries _in descending order_ of number of job listings as a comma-separated list. Then, include an 11th line as `Other`, with its total being the sum of all the jobs from the non top-10 countries.

### 7. Create a Google Bar Chart and HTML table of the states

Using `data-hold/domestic-jobcount.json`, print a webpage containing:

1. A Google interactive bar chart showing the job totals for *only the top 10 states* with the most jobs.
2. An HTML table listing the states and their job totals in _alphabetical order_.

### 8. Print out a world Google Map with the non-US countries and a table

Similar to Exercise 7, except use `data-hold/domestic-jobcount.json` and instead of printing a bar chart, use a Interactive Google Geochart to produce a map of _only_ the foreign countries that have at least 1 job.


### 9. * Get first 250 jobs in California, count full time jobs

Query the USAJobs API to get 250 job listings. From those job listings, count the number of jobs that are considered __full-time__ jobs.


### 10. Get first 250 jobs in California, get jobs available to non-citizens

Same USAJobs query as in Exercise 9, but count the jobs that are open to non-U.S. citizens.


### 11. * Find the 5 highest paying jobs in California among full-time jobs

Using a static file [TK], which will contain all of the California jobs, sort the list by the _maximum_ possible salary amount and print the 5 highest-paying jobs as a comma-separated list of: the job title, the minimum salary, the maximum salary.


### 12. Find the job with the highest spread between min and max

Using the [data dump of all California jobs](http://stash.compjour.org/data/usajobs/california-all.json) find the job with the biggest difference between minimum and maximum salary.

### 13.* Find the average number of days between post-date and end-date

Using the [data dump of all California jobs](http://stash.compjour.org/data/usajobs/california-all.json), calculate the _average number of days_ between when a job is first posted and its expiration date.

### 14. Find the jobs with the shortest date spread and the longest date spread

Similar to Exercise #13, except find the two jobs with the shortest and longest time periods between the start and end of their posting dates.


### 15. Produce an interactive Google Geochart showing jobs per California city

Difficult data munging

